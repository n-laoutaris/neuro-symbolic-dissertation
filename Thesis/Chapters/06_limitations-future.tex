\chapter{Limitations \& Future Work} \label{ch:future}
The development and evaluation of the proposed Neuro-Symbolic pipeline has highlighted several boundaries that currently exist between generative AI capabilities and the requirements of administrative automation. This chapter synthesizes both the technical and the conceptual constraints encountered during the pilot study with suggestions for future development. The goal of this last part is to define the necessary trajectory for moving from pilot-scale experimentation toward production-grade digital governance.

\section{Infrastructure Dependencies \& Digital Sovereignty} \label{subsec:api_limits}
The experiments conducted in this study were significantly shaped by their reliance on proprietary, cloud-based "Model-as-a-Service" (MaaS) infrastructure. While the use of the Google Gemini ecosystem provided reasoning capabilities adequate for a pilot-scale prototype, it introduced a significant \textit{infrastructural dependency} that reveals an important limitation for public sector applications.

\subsection{Model and Resource Constraints}
A primary limitation of this work is the narrow diversity of the models used. Due to the reliance on free-tier access, the study was restricted to just two models within a single vendor's ecosystem. This prevents a broader comparative analysis of how different architectural families (such as GPT or Llama) handle the specific nuances of administrative logic. Future research must expand to include a statistically significant variety of LLMs. This could be a way to determine if any of the inadequacies, such as the "Semantic Drift" observed in this study, are universal traits of generative AI or a specific characteristic of the utilized models.

\subsection{Operational Fragility and the Replication Crisis} 
The reliance on external APIs introduced another roadblock. Throughout the implementation phase, unannounced updates to the underlying models and shifting rate-limiting policies resulted in regressions in throughput and occasionally also logic. This lack of control over the "versioned state" of the model created a significant barrier to reproducibility as well. In a digital governance context, such operational instability is unacceptable \cite{Kuziemski2020AI}. A legal pipeline must be idempotent and resilient to the business decisions of third-party "Big Tech" providers \cite{Hanisch2023Digital}.

\subsection{Model Specialization}
A sigificant limitation of relying on models from a "foreign" (usually general-purpose) ecosystem (like Gemini or GPT) is their lack of alignment with specific legislative drafting styles \cite{Fei2023LawBench}. Future research should possibly prioritize \textit{Domain-Specific Fine-Tuning}. By taking an open-weights local model and training it on a curated dataset of adminstrative texts, together with their formal corresponding logic, we can create a specialized LLM. This transition from few-shot (or zero-shot) prompting to a fine-tuned model is expected, according to the literature \cite{Naveed2023A}, to significantly reduce the occurrence of phenomena observed in this dissertation, such as the "Language Bleed", and more importantly, to improve the model's understanding of concepts like administrative hierarchy.

\subsection{A Roadmap Toward Sovereign AI}
To address the dependencies discussed previously, the future roadmap for this research prioritizes the transition to \textit{Digital Sovereignty}. By migrating the Neuro-Symbolic pipeline from cloud-based APIs to local, open-weights models hosted on private government infrastructure, several strategic goals can be achieved:
\begin{itemize}
    \item \textbf{Data Privacy:} Full compliance with GDPR by ensuring citizen data never leaves a secure environmen.
    \item \textbf{Operational Stability:} Eliminating the risk of API-related "logic shifts" or downtime.
    \item \textbf{Fine-tuning:} The ability to perform Domain-Specific Fine-tuning on actual legislative corpuses, which is often restricted or cost-prohibitive on proprietary cloud platforms.
\end{itemize}
The move toward local infrastructure is definitely a technical upgrade but more importantly it is a stepping stone for the "No-Stop Government" vision, ensuring that the code remains under the exclusive control of the state.

\section{Scalability}
While the pilot study successfully demonstrated the technical feasibility of the Neuro-Symbolic pipeline, the scope of the experimental campaign was intentionally narrowed to prioritize analytical depth over statistical breadth. A clear path exists for scaling the framework in future research.

\subsection{Document Corpus and Prompting Strategies}
The primary limitation regarding generalizability is the limited sample size of the included document corpus. By focusing the evaluation on two administrative document sets, the study provided a high-resolution view of the pipeline's behavior in a complex scenario. However, to validate the framework's usefulness across the entire spectrum of public administration, future work must scale the experimental setup to include hundreds of heterogeneous documents across different domains (e.g., healthcare, transportation, business licensing etc.). \par
Furthermore, the prompt engineering strategies employed were primarily focused on instruction following. Future research should investigate more sophisticated strategies, such as deeper Chain-of-Thought (CoT) prompting or more complex and specific "Self-Refine" loops, to determine if these techniques can improve the success rates identified in the Pilot Study.

\subsection{From Pilot to Interoperability}
A fundamental strength of this dissertation's methodology is the decision to ground the symbolic components in established European standards, turning the synthesized graphs from isolated artifacts to being inherently interoperable with the broader European semantic ecosystem. Currently, the pipeline utilizes a subset of these vocabularies. The next phase of development should involve:
\begin{itemize}
    \item \textbf{Full Vocabulary Integration:} Expanding to cover the entirety of the CCCEV and CPSV-AP classes, allowing for the representation of the whole spectrum of the Public Service and its preconditions.
    \item \textbf{Cross-Border Interoperability:} Testing the pipeline on legislative texts from different EU member states to evaluate if the neuro-symbolic pipeline can handle multi-lingual semantics.
    \item \textbf{Automated Ontology Mapping:} Implementing a dynamic mapping stage where the LLM can identify and utilize newly added classes from the official EU vocabularies without manual updates.
\end{itemize}
By leaning heavily on these standards, the framework serves as a modular component of the "Once-Only Principle", ensuring that once a rule is synthesized and validated, it can be shared and understood by any administrative system across the European Union.

\section{Methodological Refinement}
Beyond scaling the quantity of data, future iterations of this research must refine the qualitative evaluation of the pipeline's artifacts. The current study relied on functional execution success, but if we are to gain a deeper understanding of the "Neuro-Symbolic Gap", we require more granular metrics.

\subsection{Root Cause Analysis}
While our automated testing logs provide a baseline for failure rates, they cannot fully explain the linguistic nor the logical nuances that lead to a hallucinated constraint. A necessary next step is the introduction of a human expert at the role of auditor. According to this idea, domain experts (possibly legal and administrative professionals) will review the specific cases where the pipeline failed either in syntax or logic, to categorize the errors. This will provide valuable insights such as if failures occur primarily during the precondition extraction (interpreting the text) or text-to-logic (generating the code). Understanding this divide is very important for pinpointing which stage of the pipeline requires more intensive engineering.

\subsection{Artifact Evaluation}
Currently, the evaluation of the pipeline artifacts is primarily functional. Future work should implement a method to evaluate them in depth.
\begin{itemize}
    \item \textbf{Topology vs. Content:} Instead of comparing node names or descriptions, the analyzer will use metrics such as Graph Edit Distance (GED) to evaluate how different the LLM-generated structures are between different iterations (runs) using the same configuration.
    \item \textbf{Logic Similarity Metrics:} Future frameworks should explore similarity metrics for SPARQL as well, such as Tree Edit Distance on Abstract Syntax Trees (AST), to assess how logically different the generated queries are.
    \item \textbf{Natural Language Similarity:} The stability of natural language summaries can be assessed through \textit{embeddings} to test for consistency.
    \item \textbf{Artifact Benchmarking:} Another approach to facilitate the same goal is for human experts to draft "perfect" reference graphs, summaries and SPARQL queries for a control set of documents. By comparing against these gold standards, we can measure semantic and structural drifts with mathematical precision, identifying patterns where the LLM consistently over-simplifies or over-complicates.
\end{itemize}
It is worth noting that since this work employs a meticulous persistence approach, keeping all artifacts on file, many of these ideas can be implemented already using the stored artifacts without needing to re-run experiments.

\section{Trust-Centric Optimization}
As discussed previously, in the context of public service eligibility, the cost of a False Positive (incorrectly recommending a benefit) is often higher than the cost of a False Negative (missing an opportunity for plausible eligibility). Future iterations of the pipeline must be refined to prioritize \textit{Precision} (Trustworthiness) over \textit{Recall} (Coverage). \par
The prompts should be engineered and the intermediate representations should be calibrated to be \textit{conservative by default}. If the model encounters an ambiguous clause, t should be instructed to be strict  or even possibly to flag the constraint for human review rather than attempting a probabilistic "best guess". \par
Through this lens, the JSON Information Model can be expanded to include fields like some type of "Confidence Attribute" where the model self-reports its own certainty for each extracted precondition, allowing the symbolic stage to automatically reject any logic that falls below a specific trust threshold.