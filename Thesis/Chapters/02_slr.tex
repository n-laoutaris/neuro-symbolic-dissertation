\chapter{Systematic Literature Review}
This chapter details the Systematic Literature Review (SLR) conducted to establish the theoretical foundations of Neuro-Symbolic AI.

\section{Introduction}
We approach the current state of research in Neuro-Symbolic AI, specifically focusing on how Large Language Models (LLMs) and Knowledge Graphs (KGs) are combined.
We aim to identify existing approaches for extracting rules from text and generating formal logic (SPARQL/SHACL), as well as methods of evaluating the results of such a process.

\section{Methodology}
To ensure scientific rigor and reproducibility, the review adheres to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. The process was structured into four distinct phases. First, we defined specific Research Questions (RQs). Second, we executed an automated search strategy on the Scopus database. Third, we applied a two-stage screening process: an initial practical screening of titles and abstracts, followed by a rigorous quality assessment of full texts. This phase utilized specific inclusion/exclusion criteria. Finally, data was extracted from the selected primary studies into a standardized matrix to synthesize key themes, directly related to the RQs.

\subsection{Research Questions}
To achieve our objective, we defined three specific Research Questions (RQs) that guide the data extraction and synthesis process:
\begin{itemize}
    \item \textbf{RQ1:} How are Large Language Models (LLMs) currently utilized to extract structured knowledge and conditional rules from unstructured text?
    \item \textbf{RQ2:} What are the state-of-the-art approaches for translating natural language requirements into executable constraint languages (specifically SHACL and SPARQL)?
    \item \textbf{RQ3:} What methodologies exist for evaluating the functional correctness and operational stability of LLM-generated logic?
\end{itemize}
\textbf{RQ1} explores the initial phase of the proposed pipeline (Text-to-Graph), while \textbf{RQ2} focuses on the core challenge of logic generation. \textbf{RQ3} allows us to critically analyze how existing studies ensure trust and correctness.

\subsection{Search Strategy}
To identify relevant records, we conducted an automated search on the \textit{Scopus} database. 
Scopus was selected as the source due to its extensive coverage of academic literature.
The search was executed on the 1st of December 2025.
A search query was constructed to find the intersection of Generative AI and Semantic Web technologies. We employed Boolean logic to combine three conceptual blocks:
\begin{enumerate}
    \item \textbf{Generative AI Terms:} ("Large Language Model" OR "LLM")
    \item \textbf{Target Logic/Language:} ("SHACL" OR "SPARQL")
    \item \textbf{Symbolic Context:} ("Semantic Web" OR "Knowledge Graph")
\end{enumerate}
These blocks were combined using the \texttt{AND} operator. The final search string applied to the Title, Abstract, and Keywords fields was:
\begin{quote}
\texttt{( "Large Language Model" OR "LLM" ) AND \\
( "SHACL" OR "SPARQL" ) AND \\ 
( "Semantic Web" OR "Knowledge Graph" )}
\end{quote}
We applied some metadata filters during this phase:
\begin{itemize}
    \item \textbf{Language:} Only papers written in English were considered.
    \item \textbf{Document Type:} We focused on Articles and Conference Papers, excluding trade journals and errata.
\end{itemize}
Interestingly, despite the Date Range not being restricted, all results fell in the range of years 2023--2026. This could be explained by the fact that the application of Large Language Models to formal constraint languages (like SHACL) is a nascent field that emerged primarily after the widespread adoption of GPT-4 class models.

The described search strategy yielded an initial set of candidates which were then subjected to the screening process described in the following section.

\subsection{Inclusion/Exclusion Criteria}
Next, we established a set of inclusion and exclusion criteria that reflect the focus of this review. These were applied to Titles and Abstracts during the initial "Practical Screening" phase. Table \ref{tab:criteria} summarizes the criteria used.
\begin{table}[htbp]
    \centering
    \caption{Inclusion and Exclusion Criteria}
    \label{tab:criteria}
    \renewcommand{\arraystretch}{1.4} % Adds breathing room to rows
    \resizebox{\textwidth}{!}{% Resize to fit page width
    \begin{tabular}{|p{0.15\textwidth}|p{0.4\textwidth}|p{0.4\textwidth}|}
        \hline
        \textbf{Category} & \textbf{Inclusion Criteria} & \textbf{Exclusion Criteria} \\ \hline
        
        \textbf{Task Focus} & 
        Text-to-Graph extraction, Text-to-SPARQL/SHACL generation, GraphRAG architectures. & 
        Pure NLP (summarization), low-level graph mechanics (Entity Alignment, Link Prediction, Subgraph Extraction), Dataset creation. \\ \hline
        
        \textbf{Methodology} & 
        Neuro-Symbolic architectures, Prompt Engineering for logic generation, Fine-tuning, Evaluation Frameworks for Semantic Accuracy. & 
        Traditional Machine Learning (non-generative), Reinforcement Learning without LLMs. \\ \hline
        
        \textbf{Data Flow} & 
        \textbf{Forward:} Transforming unstructured text into formal logic or structured data (Text $\rightarrow$ Logic). & 
        \textbf{Reverse:} Transforming structured data into natural language (Verbalization/Explanation). \\ \hline

        \textbf{Mode} & 
        Textual inputs with or without pre-processing. & 
        Multimodal studies (Speech/Image), Computer Vision, Temporal Data. \\ \hline
        
        \textbf{Type} & 
        Peer-reviewed Articles and Conference Papers. & 
        Conference Proceedings (Meta-entries), Posters, Editorials, Preliminary Results. \\ \hline
    \end{tabular}
    }
\end{table}

Of the papers sought, some could not be retrieved due to access restrictions (paywall). The remaining ones were downloaded assessed for eligibility by reading the full text.
In this "Quality Screening" phase, we applied a second set of quality exclusion criteria (QE):
\begin{itemize}
    \item \textbf{QE1 (Domain \& Logic Mismatch):} Articles situated in descriptive scientific domains (e.g., bioinformatics, chemistry) where the knowledge structure is purely factual/relational rather than normative or rule-based, offering low transferability to eligibility logic.
    \item \textbf{QE2 (Complexity \& Task Focus):} Sources focusing on simple factoid Question Answering (KGQA) that do not analyze the extraction or generation of complex conditional constraints (if-then-else logic) required for compliance or eligibility
    \item \textbf{QE3 (Methodological Maturity):} Studies limited to model-vs-model benchmarking or evaluation of existing datasets without proposing novel neuro-symbolic pipeline architectures or logic-validation frameworks.
\end{itemize}
The next section summarizes the results following this quality assessment.

\section{Results}
From an initial set of 125 records, 25 studies were identified as meeting all eligibility criteria.

\subsection{PRISMA Flow Diagram}
The search and screening process can be summarized in the PRISMA flow diagram (Figure \ref{fig:prisma}). 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/prisma_flow_diagram.pdf}
    \caption{PRISMA Flow Diagram of the selection process}
    \label{fig:prisma}
\end{figure}

\subsection{Data Extraction}
Table \ref{tab:extraction_matrix} presents the data extraction summary for the 25 included studies. The studies are categorized 

\begin{center}
    \small
    \begin{longtable}{|p{2.6cm}|p{2.8cm}|p{1.4cm}|p{3.5cm}|p{3.3cm}|}
        \caption{Final Synthesis Matrix of Included Studies ($n=25$)} \label{tab:extraction_matrix} \\
        \hline
        \textbf{Study} & \textbf{Core Task / Domain} & \textbf{Logic} & \textbf{Neuro-Symbolic Integration} & \textbf{Validation Method} \\ \hline
        \endhead
    
        \multicolumn{5}{|c|}{\textit{Category 1: Public Administration \& Normative Compliance}} \\ \hline
        Konstantinidis (2025) & Service Rec. (Public Admin) & SHACL & LLM extraction from PDF Laws & Human Expert Assessment \\ \hline
        Oranekwu (2026) & Cyber Compliance (IoT) & SPARQL & Ontology-driven RAG pipeline & Deterministic Reasoning \\ \hline
        Spyropoulos (2025) & Entity Mining (Police) & RDF/OWL & LLM-driven Narrative Extraction & Visual/SPARQL Verification \\ \hline
        Hanuragav (2025) & CSR Validation (Medical) & SHACL & Deterministic ETL + YAML drafting & Deterministic Execution \\ \hline
    
        \multicolumn{5}{|c|}{\textit{Category 2: Automated Logic Synthesis \& Semantic Parsing}} \\ \hline
        Agarwal (2024) & Complex QA (General) & KoPL & SymKGQA: Symbolic Program Gen. & Hits@1 and F1 Scores \\ \hline
        Avila (2025) & Scientific QA (SciQA) & SPARQL & Hybrid RAG + Few-shot ICL & F1-score on Benchmarks \\ \hline
        Jiang (2025) & Multi-KG QA (General) & SPARQL & Two-stage Sketch-based parsing & Hits@1 and F1 Scores \\ \hline
        Shah (2024) & Multi-hop QA (General) & SPARQL & Planned Query Guidance & Execution Match Accuracy \\ \hline
        Walter (2026) & Generic Reasoning (Multi) & SPARQL & Zero-shot Iterative Agent & Probabilistic (Benchmark F1) \\ \hline
        Soularidis (2024) & Rule Gen. (Search/Rescue) & SWRL & Template-driven GPT-4o Prompting & Human vs. Expert (F1) \\ \hline
        Lehmann (2023) & Semantic Parsing (Wiki) & CNL & Controlled Natural Lang. parsing & BLEU / ROUGE / Hits@1 \\ \hline
        Kovriguina (2023) & SPARQL Gen. (Fantasy) & SPARQL & SPARQLGEN: Subgraph Augment. & F1-macro on Benchmarks \\ \hline
        Mountantonakis (2025) & Cultural Heritage (Art) & SPARQL & Two-stage Path Pattern prediction & Benchmark Accuracy \\ \hline
        Ongris (2024) & Wikidata QA (General) & SPARQL & Sequential LLM Chaining (GraphRAG) & Jaccard Similarity \\ \hline
        Vieira da Silva (2024) & Capability Model. (Manuf.) & OWL & TBox-contextualized prompting & OWL Reasoning/SHACL \\ \hline
        Emonet (2025) & Federated QA (Bio) & SPARQL & ShEx/VoID-driven RAG retrieval & Execution Success Rate \\ \hline
        Mashhaditafreshi (2025) & Digital Twins (IoT) & SHACL & SAMM Copilot: Iterative bootstrapping & Automated Jena Checks \\ \hline
    
        \multicolumn{5}{|c|}{\textit{Category 3: Evaluation, Stability \& Trustworthiness}} \\ \hline
        Sequeda (2025) & Enterprise Trust (SQL) & SPARQL & Position Paper on KG-based trust & Execution Benchmarks \\ \hline
        Allemang (2024) & Query Correction (SQL) & SPARQL & Ontology-based Check + Repair & Iterative LLM Repair \\ \hline
        Gashkov (2025) & Query Filtering (General) & SPARQL & Instruction-Tuned LLM-as-a-Judge & Answer Trustworthiness \\ \hline
        Adam (2025) & Statement Verif. (Bio) & RDF & RAG using External Snippets & Precision / Recall \\ \hline
        Meyer (2025) & KGE Benchmarking (Web) & SPARQL & LLM-KG-Bench 3.0 Framework & Automated Syntax/F1 \\ \hline
        Kosten (2024) & Prompt Eng. (General) & SPARQL & Multi-framework Evaluation & Execution Accuracy \\ \hline
        Schmidt (2026) & Systematicity (Wiki) & SPARQL & CompoST: Compositional Testing & Compositionality F1 \\ \hline
        Tufek (2025) & Artifact Valid. (Industry) & SPARQL & Zero-shot Instruction Prompting & Domain-specific F1 Score \\ \hline
    
        \hline
    \end{longtable}
    \end{center}

\section{Thematic Analysis}

\subsection{Neuro-Symbolic Pipelines in Public Administration}
In the domain of public administration and compliance, research is focused on translating high-stakes text (laws, regulations) into logic. However, these systems are still largely conceptual prototypes.

\subsection{State-of-the-art in Logic Synthesis}
The broader field of automated logic synthesis is dominated by descriptive question-answering tasks. While methods like 'Planned Query Guidance' improve accuracy, they are rarely tested against prescriptive legal constraints.

\subsection{Methodologies for Logic Validation and Trust}
Evaluation frameworks are shifting toward 'Knowledge Graphs as a Source of Trust'. Yet, current validation relies on probabilistic 'LLM-judges' or single benchmarks, leaving a gap for the deterministic testing required in rigorous governance environments.

\section{Discussion and Research Gap}
While the literature shows success in extraction and generation, there is a gap in deterministic validation. no study has yet implemented a comprehensive Mutation Testing framework to rigorously test the structural stability of LLM-generated SHACL shapes for public service eligibility.

Other idea: most literature is about KGQA and information retrieval. Nobody (almost?) tries to make the sparql queries represent RULES and not just questions. 

Other idea: The use of SHACL. Who uses it and how? Argument in favor of it being a good idea (unified graph, automation, explainability?)