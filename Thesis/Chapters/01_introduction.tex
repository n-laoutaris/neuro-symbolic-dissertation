\pagenumbering{arabic} 

\chapter{Introduction}
The digital transformation of the public sector is ongoing, and at the present time, it has begun to transition from its simple initial goal of mere digitization, towards the more ambitious vision of proactive governance \cite{Dunleavy2005New}. According to this concept, often described in modern literature as the "No-Stop Government" \cite{SCHOLTA201911}, administrative services are to be delivered automatically. This would mean, for example, that the identification of eligible citizens and the facilitation of their rights would not require manual intervention neither repetitive data submission from them. \par
However, significant technological bottlenecks persist. Legislative rules, for instance, remain locked away in unstructured natural language documents, creating a bureaucratic maze that is often very troublesome to navigate \cite{Manny2021Barriers}. As long as this phenomenon persists, the vision of a proactive government remains a theoretical ideal, leaving citizens struggling with being disconnected from the support systems and entitlements they are legally owed. \par
This dissertation is inspired by the need to dismantle this maze and empower citizens with effortless access to their rights. As we explore this path, it becomes evident that this is a high-stakes engineering challenge. Thinking of tools at our disposal, when it comes to document understanding and extracting knowledge from natural language, Generative AI naturally comes to mind. However, this challenge cannot rely on \textit{approximate} AI assistance. A true attempt to tackle this issue will require an airtight, interoperable, explainable and rigorously tested solution. Thus, this work addresses the challenge by proposing a novel \textit{Neuro-Symbolic pipeline} within a pilot study that establishes the technical requirements for automating the synthesis of machine-readable administrative logic derived from text. \par
Conceptually, the system follows the following workflow. First, it uses the interpretive capability of LLMs to extract meaning from text. Then, it transforms that meaning into intermediate representations like structured data objects and Knowledge Graphs. Finally, it generates executable logic in the form of a SHACL (Shapes Constraint Language) graph. The result is a deterministic mapping between natural language legislation and executable SHACL shapes. \par
To evaluate the feasibility of this approach and its potential for real-world application, the system is implemented and stress-tested within the context of a \textit{Public Service Recommender System}, designed to provide citizens with legally-grounded eligibility recommendations. Through this implementation, we explore the boundaries of current generative AI in the high-stakes domain of public administration and its potential to be integrated with semantic technologies.

\section{Background and Motivation}
Public administration systems worldwide are characterized by an ever-increasing volume of regulatory documents \cite{Adam2017Rule}. These texts define the parameters of social welfare, tax obligations, civic entitlements and more. Currently, the primary burden of interpretation falls on the citizen. In the context of public services, determining whether one's attributes satisfy a set of legal preconditions requires significant time, effort and legal literacy. As reported by many citizens, this task is far from trivial \cite{DÃ¶ring2022Mitigating}. The result is increased \textit{administrative friction}, which is an issue for both the citizen and the government side. On the one hand, citizens (often the most vulnerable, or the most in need) have trouble navigating the maze of requirements. On the other hand, administrative staff are burdened with applying immutable rules to thousands of heterogeneous cases. \par
The emergence and continuous growth of Large Language Models has from early on been an opportunity for the field of digital governance. Unlike traditional hard-coded, rule-based algorithms, LLMs have the capability to process raw legal text and extract complex semantic relationships \cite{Bakker2025Semantic}. However, the use of generative AI in this context is stifled by the (well-documented) risk of hallucinations and lack of determinism. In a legal environment, an \textit{approximate} answer is insufficient. Administrative acts are bound to require high precision and, perhaps more importantly, an auditable "paper trail" that can be followed back to the respective source. \par
The importance of this work lies primarily in its attempt to relieve this tension. By developing a methodology that prioritizes separation between the \textit{interpretation} phase of Generative AI and the \textit{execution} phase of logic, this research provides a blueprint for safe and accountable legal automation. Technology here acts as an equalizer, ensuring that the "logic of the law" is transparent, machine-readable and grounded in interoperable cross-border standards. This is undoubtedly a technical upgrade, but furthermore, it is a requirement for \textit{administrative justice}, ensuring that every citizen can effortlessly discover and claim the opportunities they are entitled to, without being hindered by the (necessary) complexity of the state's bureaucracy.

\section{Problem Statement}
The primary challenge addressed in this research is the \textit{gap} that exists between the unstructured, often ambiguous by nature legislative text, and the strict deterministic requirements of administrative execution logic. We have established that the simple usage of AI, generative or otherwise, is insufficient for a domain where errors potentially have legal consequences. Current systems struggle due to three main obstacles:
\begin{itemize}
    \item \textbf{Reliability:} Large Language Models, while capable of interpreting text, are inherently non-deterministic and prone to hallucinations. This makes them unsuitable for direct, unsupervised generation of legal code \cite{Siino2025Exploring}. 
    \item \textbf{Complexity:} Formal constraint languages such as SHACL and SPARQL require exact syntactic precision. Even minor "token-Level" hallucinations or cross-contamination from other programming languages render the resulting logic non-executable.
    \item \textbf{Interoperability:} Most existing automation attempts result in isolated, proprietary logic structures \cite{Loutsaris2020Legal}. In the European digital landscape, however, failure to make automated rules interoperable with cross-border standards (such as the formal EU vocabularies) prevents the technical scalability required for a unified digital administration. 
\end{itemize}
Without a structured methodology to addres these roadblocks, the vision of a proactive government remains hindered by the manual error-prone labour of human-encoded rules and leaves the system vulnerable both to the limitations of traditional software development and the trust barrier of black-box AI.

\section{Objectives}
The primary aim of this dissertation is to design and evaluate a Neuro-Symbolic Pipeline capable of transforming unstructured public service regulations into executable logic, leaving an auditable trail of interoperable artifacts. To achieve this, the following objectives were defined:
\begin{enumerate}
    \item \textbf{Establish Theoretical Foundation:} To conduct a systematic literature review of existing Neuro-Symbolic technologies, identifying recurring patterns and research gaps. It is expected that this research will base this dissertation in established academic methods while avoiding known logical and architectural pitfalls that have hindered previous attempts.
    \item \textbf{Develop the Pipeline:} To construct a multi-stage "Text-to-Graph-to-Logic" pipeline that utilizes Large Language Models for semantic extraction and intermediate structured representations (JSON/RDF) for architectural grounding.
    \item \textbf{Prioritize Interoperability:} To see if there is a way to ground the synthesized logic in established European semantic standards, specifically the \textit{Core Public Service Vocabulary Application Profile} (CPSV-AP) and the \textit{Core Criterion and Core Evidence Vocabulary} (CCCEV). If this attempt succeeds, it will offer future compatibility with established e-government frameworks that function cross-border.
    \item \textbf{Test and Verify:} To develop a deterministic \textit{Mutation Testing} framework, its goal being to stress-test the generated SHACL shapes against a collection of scenarios. This would provide a quantifiable metric of functional logic accuracy.
    \item \textbf{Analyze Performance:} To conduct an experiment, of scale as large as possible for the scope of this dissertation. Benchmarking of different model architectures and prompting strategies will be required, to identify any limitations of current generative AI in this context.
    \item \textbf{Assess Feasibility:} To evaluate the reliability of this system as an end-to-end Public Service Recommender System. This will be done by taking a particular interest in analyzing how accurately it can guide a citizen, minimizing the (trust-eroding) risks of False Positives and increasing the incentives of developing such a system by multiplying True Positives.
\end{enumerate}

\section{The Recommender Context} \label{sec:recommender_context}
The symbolic logic generated by the proposed pipeline is intended to function as the decision engine of a proactive Public Service Recommender System, following the conceptual framework proposed by Konstantinidis et al. \cite{Konstantinidis2025}, summarized below. \par
In this vision, the recommender is embedded within a unified citizen portal that adheres to the Once-Only Principle (OOP), whereby public administrations reuse citizen data already held in authoritative registries rather than repeatedly requesting it from users. Under appropriate legal and technical safeguards, the portal has access to a citizen's existing data (e.g., income, family status, employment), represented as an RDF graph. \par
Rather than requiring citizens to actively search for applicable services, the recommender periodically executes the synthesized SHACL constraint shapes against the citizen data graphs. For each public service, the validation process produces a SHACL conformance report. If the report indicates eligibility, the system may proactively notify the citizen that they appear to satisfy the eligibility conditions for that service, inviting them to verify the information and initiate a formal application. \par
Within this broader system architecture, the scope of this dissertation is deliberately limited. It does not aim to design or implement a full recommender system or user interface. Instead, it addresses a critical reliability bottleneck identified in prior work: ensuring that the executable eligibility logic used by such systems is a faithful, logically correct and auditable representation of the underlying legislation. The Neuro-Symbolic pipeline and evaluation framework proposed here are motivated precisely by this requirement.

\section{Dissertation Structure}
Below is the way the remainder of this dissertation is organized:

\textbf{Chapter \ref{ch:slr}} is a Systematic Literature Review to establish the theoretical foundations needed to implement the proposed architecture.

\textbf{Chapter \ref{ch:pilot_study}} presents the implementation of the proposed Neuro-Symbolic pipeline and the ideas behind it. There are detailed descriptions of the multi-stage extraction process and the mutation testing framework used for validation.

\textbf{Chapter \ref{ch:results}} lists the quantitative results taken from the experimental phase. To analyze them, we measure the system's performance across syntactic and logical performance indicators.

\textbf{Chapter \ref{ch:discussion}} synthesizes these results and gives a subjective opinion and analysis of them, while attempting to tie those in with broader implications.

\textbf{Chapter \ref{ch:future}} summarizes the research findings, addresses the identified limitations and outlines a strategic roadmap for future work.

The source code for the Pilot Study implementation can be found in the dissertation's GitHub repository \cite{repo}, along with the \texttt{TeX} source of the present document, under the "Thesis" directory.