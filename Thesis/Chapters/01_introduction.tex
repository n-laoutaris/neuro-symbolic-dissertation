\pagenumbering{arabic} 

\chapter{Introduction}
The ongoing digital transformation of the public sector has began to transition from a simple goal of digitization toward a more ambitious vision of proactive governance \cite{Dunleavy2005New}. In this paradigm, often described as "No-Stop Government" \cite{SCHOLTA201911}, administrative services are intended to be delivered automatically, identifying eligible citizens and facilitating their rights without requiring manual intervention or repetitive data submission from them. However, significant technological and social bottlenecks persist. Legislative rules remain locked in unstructured natural language, creating an impenetrable bureaucratic maze \cite{Manny2021Barriers}. As long as this persists, the vision of a proactive government remains a theoretical ideal, leaving citizens to struggle with remaining disconnected from the support systems and entitlements they are legally owed. \par
This dissertation is inspired by the need to dismantle this maze and empower citizens with effortless access to their rights. However, as we explore this path, it becomes evident that this is a high-stakes engineering challenge that cannot rely on "approximate" AI assistance. A true attempt to tackle this issue will require an airtight, interoperable and rigorously tested infrastructure. Thus, this work addresses the challenge by proposing a novel \textit{Neuro-Symbolic pipeline}. \par
Conceptually, the system follows this workflow: it uses the interpretive power of LLMs to extract meaning from text, it transforms that meaning into intermediate Knowledge Graphs, and finally it synthesizes deterministic logic in the form of Shapes Constraint Language (SHACL) for execution. The result is the creation of a trustworthy bridge between the dense prose of a legal text and the citizen's need for certainty. \par
To evaluate the feasibility of this approach, the system is implemented and stress-tested within the context of a \textit{Public Service Recommender System}, designed to provide citizens with legally-grounded eligibility insights. Through this implementation, we explore the boundaries of current generative AI in the high-stakes domain of public administration and its potential to be integrated with semantic technologies.

\section{Background and Motivation}
Public administration systems worldwide are characterized by an ever-increasing volume  regulatory documents \cite{Adam2017Rule}. These texts define the parameters of social welfare, tax obligations, civic entitlements and more. Currently, the primary burden of interpretation falls on the citizen. Determining whether one's attributes satisfy a set of legal preconditions requires significant time, legal literacy and administrative effort, which makes the task often non-trivial for many citizens \cite{DÃ¶ring2022Mitigating}. This results in significant \textit{administrative friction}, where citizens (often the most vulnerable, the most in need) struggle to navigate the maze of eligibility requirements, while at the same time administrative staff face the burden of applying rigid rules to thousands of heterogeneous cases. \par
The emergence of Large Language Models has presented an opportunity for the field of digital governance. Unlike traditional hard-coded, rule-based algorithms, LLMs possess the linguistic fluency required to process raw legal text and extract complex semantic relationships \cite{Bakker2025Semantic}. However, the use of generative AI in this context is hampered by the (well-documented) risk of hallucinations and the lack of mathematical determinism. In a legal environment, an "approximate" answer is insufficient. Administrative acts require absolute precision and, more importantly, an audit trail that can be followed back to the source legislation. \par
The importance of this work lies in its attempt to resolve this tension. By developing a methodology that decouples the "interpretive" phase of Generative AI from the "executable" phase of logic, this research provides a blueprint for safe and accountable legal automation. Technology here acts as an equalizer, ensuring that the "logic of the law" is transparent, machine-readable and grounded in interoperable cross-border standards. This is undoubtedly a technical upgrade, but furthermore, it is a requirement for \textit{administrative justice}, ensuring that every citizen can effortlessly discover and claim the opportunities they are entitled to, without being hindered by the (necessary) complexity of the state's bureaucracy.

\section{Problem Statement}
The primary challenge addressed in this research is the \textit{gap} that exists between the unstructured, often ambiguous by nature legislative text, and the strict deterministic requirements of administrative execution logic. We have identified that simply "using AI" is insufficient for a domain where errors potentially have legal consequences. Current systems struggle to bridge this gap due to three main obstacles:
\begin{itemize}
    \item \textbf{Reliability:} Large Language Models, while capable of interpreting text, are inherently non-deterministic and prone to hallucinations. This makes them unsuitable for direct, unsupervised generation of legal code \cite{Siino2025Exploring}. 
    \item \textbf{Complexity:} Formal constraint languages such as SHACL and SPARQL require exact syntactic precision. Even minor "Token-Level Hallucinations" or "Language Bleed" (cross-contamination from other programming languages) render the resulting logic non-executable. 
    \item \textbf{Interoperability:} Most existing automation attempts result in isolated, proprietary logic structures \cite{Loutsaris2020Legal}. In the context of the European digital landscape, failure to make automated rules interoperable with cross-border standards (such as the formal EU vocabularies) prevents the technical scalability required for a truly unified digital administration. 
\end{itemize}
Without a structured methodology to bridge these domains, the vision of a proactive government remains hindered by the manual labour of human-encoded rules and leaves the system vulnerable both to the limitations of traditional software development and the trust barrier of black-box AI.

\section{Objectives}
The primary aim of this dissertation is to design and evaluate a Neuro-Symbolic Pipeline capable of transforming unstructured public service regulations into executable logic, leaving an auditable trail of interoperable artifacts. To achieve this, the following objectives were defined:
\begin{enumerate}
    \item \textbf{Establish Theoretical Foundation:} To conduct a systematic literature review of existing neuro-symbolic technologies, identifying recurring patterns and research gaps to ground this mission in established academic methods while avoiding known logical and architectural pitfalls that have hindered previous attempts.
    \item \textbf{Develop the Pipeline:} To construct a multi-stage "Text-to-Graph-to-Logic" pipeline that utilizes Large Language Models for semantic extraction and intermediate structured representations (JSON/RDF) for architectural grounding.
    \item \textbf{Ensure Interoperability:} To ground the synthesized logic in established European semantic standards, specifically the \textit{Core Public Service Vocabulary Application Profile} (CPSV-AP) and the \textit{Core Criterion and Evidence Vocabulary} (CCCEV), for compatibility with cross-border e-government frameworks.
    \item \textbf{Test and Verify:} To develop a deterministic \textit{Mutation Testing} framework capable of stress-testing generated SHACL shapes against a library of heterogeneous citizen scenarios, thereby providing a quantifiable measure of functional logic accuracy.
    \item \textbf{Analyze Performance:} To conduct a large-scale experiment, benchmarking different model architectures and prompting strategies to identify the limitations of current generative AI in this context.
    \item \textbf{Assess Feasibility:} To evaluate the end-to-end reliability of the system as a public service recommender, specifically analyzing how accurately it can guide a citizen, by minimizing the trust-eroding risks of False Positives and increasing the incentives of developing such a system by multiplying True Positives.
\end{enumerate}

\section{Dissertation Structure}
The remainder of this dissertation is organized as follows:

\textbf{Chapter \ref{ch:slr}} presents a Systematic Literature Review of the intersection between Large Language Models and Knowledge Graphs, establishing the theoretical foundations for the proposed architecture.

\textbf{Chapter \ref{ch:pilot_study}} details the methodology and technical architecture of the Neuro-Symbolic pipeline, describing the multi-stage extraction process and the mutation-testing framework used for validation.

\textbf{Chapter \ref{ch:results}} provides the quantitative findings from the experimental phase, analyzing the system's performance across syntactic and logical performance indicators.

\textbf{Chapter \ref{ch:discussion}} synthesizes these results to evaluate the broader implications of the study, identifying phenomena that currently limit administrative automation.

\textbf{Chapter \ref{ch:future}} summarizes the research findings, addresses the identified limitations and outlines a strategic roadmap for future work.

The source code for the Pilot Study implementation can be found in the dissertation's GitHub repository \cite{repo}, along with the \texttt{TeX} source of the present document, under the "Thesis" directory.