\pagenumbering{arabic} 

\chapter{Introduction}
The digital transformation of the public sector is ongoing, and in the present time, it has began to transition from its simple initial goal of mere digitization, toward the more ambitious vision of proactive governance \cite{Dunleavy2005New}. According to this concept, often described in modern literature as "No-Stop Government" \cite{SCHOLTA201911}, administrative services are to be delivered automatically. This would mean that the identification of eligible citizens and facilitation of their rights would not require manual intervention neither repetitive data submission from them. However, significant technological bottlenecks persist. Legislative rules remain locked in unstructured natural language, creating an impenetrable bureaucratic maze \cite{Manny2021Barriers}. As long as this persists, the vision of a proactive government remains a theoretical ideal, leaving citizens to struggle with remaining disconnected from the support systems and entitlements they are legally owed. \par
This dissertation is inspired by the need to dismantle this maze and empower citizens with effortless access to their rights. However, as we explore this path, it becomes evident that this is a high-stakes engineering challenge that cannot rely on "approximate" AI assistance. A true attempt to tackle this issue will require an airtight, interoperable and rigorously tested infrastructure. Thus, this work addresses the challenge by proposing a novel \textit{Neuro-Symbolic pipeline} within a pilot study that establishes the technical requirements for automating the synthesis of machine-readable administrative logic. \par
Conceptually, the system follows the following workflow. First, it uses the interpretive capability of LLMs to extract meaning from text. Then, it transforms that meaning into intermediate Knowledge Graphs. Finally, it generates executable logic in the form of a SHACL (Shapes Constraint Language) graph. The result is a deterministic mapping between natural language legislation and executable SHACL shapes. \par
To evaluate the feasibility of this approach, the system is implemented and stress-tested within the context of a \textit{Public Service Recommender System}, designed to provide citizens with legally-grounded eligibility recommendations. Through this implementation, we explore the boundaries of current generative AI in the high-stakes domain of public administration and its potential to be integrated with semantic technologies.

\section{Background and Motivation}
Public administration systems worldwide are characterized by an ever-increasing volume  regulatory documents \cite{Adam2017Rule}. These texts define the parameters of social welfare, tax obligations, civic entitlements and more. Currently, the primary burden of interpretation falls on the citizen. Determining whether one's attributes satisfy a set of legal preconditions requires significant time, effort and legal literacy. For many citizens, this is reported to make the task non-trivial \cite{DÃ¶ring2022Mitigating}. The outcome is increased \textit{administrative friction}, which is an issue for both sides. On the one hand, citizens (often the most vulnerable, or the most in need) have trouble navigating the maze of requirements. On the other hand, administrative staff are burdened with applying immutable rules to thousands heterogeneous cases. \par
The emergence and continuous growth of Large Language Models has from early on been an opportunity for the field of digital governance. Unlike traditional hard-coded, rule-based algorithms, LLMs have the capability to process raw legal text and extract complex semantic relationships \cite{Bakker2025Semantic}. However, the use of generative AI in this context is stifled by the (well-documented) risk of hallucinations and the lack of determinism. In a legal environment, an "approximate" answer is insufficient. Administrative acts are bound to require high precision and, perhaps more importantly, an auditable "papertrail" that can be followed back to the respective source. \par
The importance of this work lies in its attempt to resolve this tension. By developing a methodology that prioritizes separation between the "interpretation" phase of Generative AI from the "execution" phase of logic, this research provides a blueprint for safe and accountable legal automation. Technology here acts as an equalizer, ensuring that the "logic of the law" is transparent, machine-readable and grounded in interoperable cross-border standards. This is undoubtedly a technical upgrade, but furthermore, it is a requirement for \textit{administrative justice}, ensuring that every citizen can effortlessly discover and claim the opportunities they are entitled to, without being hindered by the (necessary) complexity of the state's bureaucracy.

\section{Problem Statement}
The primary challenge addressed in this research is the \textit{gap} that exists between the unstructured, often ambiguous by nature legislative text, and the strict deterministic requirements of administrative execution logic. We have identified that simply "using AI" is insufficient for a domain where errors potentially have legal consequences. Current systems struggle due to three main obstacles:
\begin{itemize}
    \item \textbf{Reliability:} Large Language Models, while capable of interpreting text, are inherently non-deterministic and prone to hallucinations. This makes them unsuitable for direct, unsupervised generation of legal code \cite{Siino2025Exploring}. 
    \item \textbf{Complexity:} Formal constraint languages such as SHACL and SPARQL require exact syntactic precision. Even minor "Token-Level Hallucinations" or "Language Bleed" (cross-contamination from other programming languages) render the resulting logic non-executable. 
    \item \textbf{Interoperability:} Most existing automation attempts result in isolated, proprietary logic structures \cite{Loutsaris2020Legal}. In the European digital landscape, however, failure to make automated rules interoperable with cross-border standards (such as the formal EU vocabularies) prevents the technical scalability required for a unified digital administration. 
\end{itemize}
Without a structured methodology to addres these roadblocks, the vision of a proactive government remains hindered by the manual labour of human-encoded rules and leaves the system vulnerable both to the limitations of traditional software development and the trust barrier of black-box AI.

\section{Objectives}
The primary aim of this dissertation is to design and evaluate a Neuro-Symbolic Pipeline capable of transforming unstructured public service regulations into executable logic, leaving an auditable trail of interoperable artifacts. To achieve this, the following objectives were defined:
\begin{enumerate}
    \item \textbf{Establish Theoretical Foundation:} To conduct a systematic literature review of existing Neuro-Symbolic technologies, identifying recurring patterns and research gaps. It is expected that this research will base this dissertation in established academic methods while avoiding known logical and architectural pitfalls that have hindered previous attempts.
    \item \textbf{Develop the Pipeline:} To construct a multi-stage "Text-to-Graph-to-Logic" pipeline that utilizes Large Language Models for semantic extraction and intermediate structured representations (JSON/RDF) for architectural grounding.
    \item \textbf{Prioritize Interoperability:} To see if there is a way to ground the synthesized logic in established European semantic standards, specifically the \textit{Core Public Service Vocabulary Application Profile} (CPSV-AP) and the \textit{Core Criterion and Core Evidence Vocabulary} (CCCEV). If this attempt succeeds, it will offer future compatibility with established e-government frameworks that function cross-border.
    \item \textbf{Test and Verify:} To develop a deterministic \textit{Mutation Testing} framework, its goal being to stress-test the generated SHACL shapes against a collection of scenarios. This would provide a quantifiable metric of functional logic accuracy.
    \item \textbf{Analyze Performance:} To conduct an experiment, of scale as large as possible for the scope of this dissertation. Benchmarking of different model architectures and prompting strategies will be required, to identify any limitations of current generative AI in this context.
    \item \textbf{Assess Feasibility:} To evaluate the reliability of this system as an end-to-end public service recommender system. This will be done by taking a particular interest in analyzing how accurately it can guide a citizen, minimizing the (trust-eroding) risks of False Positives and increasing the incentives of developing such a system by multiplying True Positives.
\end{enumerate}

\section{Dissertation Structure}
The rest of the dissertation is organized in the following way:

\textbf{Chapter \ref{ch:slr}} contains a Systematic Literature Review of the union between Large Language Models and the Semantic Web, as it is necessary to establish theoretical foundations before implementing the proposed architecture.

\textbf{Chapter \ref{ch:pilot_study}} details the methodology and technical architecture of the Neuro-Symbolic pipeline, describing the multi-stage extraction process and the mutation-testing framework used for validation.

\textbf{Chapter \ref{ch:results}} provides the quantitative findings from the experimental phase, analyzing the system's performance across syntactic and logical performance indicators.

\textbf{Chapter \ref{ch:discussion}} synthesizes these results and gives a subjective opinion and analysis of them, while attempting to tie those in with broader implications.

\textbf{Chapter \ref{ch:future}} summarizes the research findings, addresses the identified limitations and outlines a strategic roadmap for future work.

The source code for the Pilot Study implementation can be found in the dissertation's GitHub repository \cite{repo}, along with the \texttt{TeX} source of the present document, under the "Thesis" directory.