\chapter{Pilot Study}
This chapter details the design, implementation and experimental testing of a novel Neuro-Symbolic pipeline for modeling automating public service eligibility checks.

\section{Overview}
The proposed architecture addresses the limitations of "black-box" Large Language Models \cite{Lin2023Generating} by enforcing a strict separation between neural interpretation (extracting meaning from text) and symbolic execution (validating logic against data). \par
The methodology is structured around a "Text-to-Graph-to-Logic" workflow. The system transforms unstructured administrative documents into formal Knowledge Graphs and executable SHACL shapes through a chain of intermediate structured representations. This design prioritizes explainability and determinism, ensuring that the final eligibility decision is derived from explicit, auditable rules rather than probabilistic approaches. \par
This chapter is organized as follows: Section \ref{subsec:semantic_modelling} defines the semantic schemas that ground the system. Section \ref{subsec:pipeline} details the four-stage extraction and generation pipeline. Section \ref{subsec:validation_engine} describes the validation engine, and Section \ref{sec:experimental_design} outlines the experimental framework used to stress-test the system's capabilities through automated mutation testing.

\section{Methodology and System Architecture}

\subsection{Setup Environment}
The pipeline was implemented using Python 3.12.9, utilizing a modular architecture to separate core processing logic from experimental orchestration. The system relies on local processing for semantic graph operations and cloud-based APIs for interfacing with Large Language Models.

\subsubsection{System Architecture} \label{subsec:system_architecture}
The codebase follows a functional separation of concerns, organized into three distinct layers: 
\begin{enumerate} 
    \item \textbf{The Core Logic Layer:} A modular custom Python library encapsulating the functional logic of the system. It contains the reusable logic, such as API communication, graph operations, parsing and testing utilities. It also contains the end-to-end extraction-generation workflow, dubbed the \textit{pipeline core}.
    \item \textbf{The Orchestration Layer:} An interactive Jupyter Notebook serves as the control interface. This layer manages the experimental loop, injects configuration variables into the core modules and handles exceptions without interrupting the iterative execution of experiments.
    \item \textbf{The Persistence Layer:} To ensure auditability and reproducibility, the system employs a meticulous "Artifact Preservation" strategy. Every experimental run generates a dedicated directory locally, containing all intermediate outputs of the core pipeline. Furthermore, during testing, metrics and metadata are saved in a Master CSV file for post-hoc analysis.
\end{enumerate}

\subsubsection{Technologies and Libraries}
The system combines standard Semantic Web technologies with modern Data Science tools:
\begin{itemize} 
    \item \textbf{RDFLib:} Used for parsing, manipulating and serializing RDF graphs (in Turtle format), as well as executing local SPARQL queries.
    \item \textbf{PySHACL:} The standard Python implementation of the SHACL validation engine, used to validate the LLM-generated shapes against the citizen data.
    \item \textbf{Pydantic:} Employed for schema enforcement of the intermediate JSON representations. It ensures that the unstructured extractions from the LLM adhere to strict data types and structural constraints, acting as a structural "gatekeeper".
    \item \textbf{Pandas:} Used for the post-hoc aggregation and analysis of the testing logs.
\end{itemize}

\subsection{Semantic Data Modeling} \label{subsec:semantic_modelling}
This pipeline was designed specifically with public service documents in mind. To bridge the gap between unstructured administrative text and deterministic validation logic, two distinct semantic layers were defined. These RDFS schemas serve as the symbolic "grounding" for the Large Language Model.

\subsubsection{The Public Service Meta-Model} \label{subsec:service_schema}
To ensure semantic interoperability and standardization, the modeling of the public service itself adheres to European formal vocabularies, specifically the Core Public Service Vocabulary Application Profile (CPSV-AP) and the Core Criterion and Evidence Vocabulary (CCCEV). The schema follows the following hierarchical structure: 
\begin{itemize} 
    \item \textbf{cpsv:PublicService}: The root node representing the public service itself. 
    \item \textbf{cccev:Constraint}: Connected to the root node via \texttt{cpsv:holdsRequirement}, these nodes represent individual preconditions extracted from the text. 
    \item \textbf{cccev:InformationConcept}: These nodes are connected to Constraint nodes via \texttt{cccev:constrains} and represent the abstract information required to evaluate a constraint. 
\end{itemize} 
The adoption of established EU standards is a deliberate architectural choice, made to ensure cross-border interoperability and extensibility \cite{Stani2020How}. By anchoring the pipeline's output in the CPSV-AP and CCCEV ecosystems, the generated graphs are compatible with the broader European e-Government infrastructure (such as the Single Digital Gateway). Furthermore, this modular design allows for future expansion where the pipeline could automatically ingest the full breadth of these ontologies (complex Evidence mappings, Agent definitions, Output representations), without requiring a fundamental restructuring of the core logic.

\subsubsection{Citizen Schema} \label{subsec:citizen_schema}
While the Public Service Meta-Model describes the \textit{rules}, the Citizen Schema describes the \textit{applicant}. This work utilizes a domain-specific RDFS schema tailored to the requirements of each document and generated in a separate workflow (not presented here) by the same LLM used in the implementation of the rest of the pipeline. The model is instructed to use granular instead of aggregate data as nodes (e.g. prefer "Date of Birth" rather than "Age") and is encourgaed to use abstract and reusable classes. \par
It has been demonstrated that the generation of such schemas can be automated as part of the pipeline \cite{Konstantinidis2025}. However, for the scope of this pilot study, the Citizen Schema is treated as fixed input context. This methodological choice serves two purposes: 
\begin{enumerate} 
    \item \textbf{Experimental Control:} By fixing the target schema, we isolate the performance of the LLM in \textit{logic generation} (SHACL/SPARQL) and \textit{extraction}, without the confounding variable of schema generation errors.
    \item \textbf{Prerequisite for Testing:} The automated testing framework that will be presented relies on injecting specific faults into the citizen graph (e.g., modifying property values to trigger violations). This requires a schema structure that is known in advance. Had the schema been generated dynamically during each run, it would be impossible to define a library of test scenarios targeting specific graph nodes. 
\end{enumerate}

\subsection{The Extraction and Generation Pipeline} \label{subsec:pipeline}
The core contribution of this work is the following multi-stage, Neuro-Symbolic pipeline. The process follows a sequential data flow, depicted in Figure \ref{fig:pipeline}, consisting of four primary stages.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Pipeline.png}
    \caption{Flow Chart of the core pipeline}
    \label{fig:pipeline}
\end{figure}

\subsubsection{Stage 1: Document Summarization and Preconditions Extraction} 
The pipeline begins with the ingestion of the raw public service document (PDF). Using a Large Language Model, the unstructured text is processed to extract a summary of eligibility preconditions. The prompt is designed to filter out administrative noise and standardize the format of the rules. Since this is the first and most vital piece of information to be passed downstream, summarization is necessary as it reduces the cognitive load required for the subsequent logic generation steps.

\subsubsection{Stage 2: Information Model Generation}
In this critical neuro-symbolic step, the extracted preconditions are transformed into a structured JSON "Information Model". The Information Model organizes the unstructured list of rules into a strict hierarchy that mirrors the Meta-Model structure: 
\begin{itemize} 
    \item \textbf{Constraints:} Each eligibility rule is encapsulated as a Constraint object, containing the natural language description of the rule. 
    \item \textbf{Information Concepts:} Nested within each Constraint are the abstract Information Concepts, representing the specific pieces of evidence or data required to evaluate that rule. 
\end{itemize}
Inferring these concepts from the list of preconditions is the main reasoning task of the LLM at this stage. However, a second task it is prompted with is to act as a semantic mapper. The LLM is provided with the Citizen Schema (defined in Section \ref{subsec:citizen_schema}) as a strict vocabulary constraint to prevent the hallucination of non-existent properties. With it, it is instructed to connect each Information Concept with a number of Citizen nodes, by constructing specific traversal paths through the ontology (e.g., mapping the concept of "Applicant Age" to the path \texttt{:Applicant $\rightarrow$ :birthDate}). This method has been shown to potentially increase the consistency and reliability of the produced results \cite{Pan_2024}. \par
The output is strictly enforced using a Pydantic schema definition, ensuring valid JSON structure. The resulting artifact effectively creates a "blueprint" for downstream tasks. It contains all the necessary semantic links to be deterministically serialized into valid RDF triples in the subsequent stage, while ensuring that all data references are grounded in the controlled vocabulary of the Citizen Schema. Listing \ref{lst:json-model} provides an example snippet of the JSON Information Model.
\lstinputlisting[
    language=json, 
    caption={JSON Information Model snippet}, 
    label={lst:json-model}
]{Snippets/info_model.json}

\subsubsection{Stage 3: Semantic Graph Construction}
Once the Information Model is established, the system deterministically (via Python code) constructs two RDF artifacts without further LLM inference:
\begin{enumerate} 
    \item \textbf{The Service Graph:} A formal representation of the public service using the CPSV-AP and CCCEV vocabularies and following the Meta-Model schema defined in Section \ref{subsec:service_schema}.
    \item \textbf{The Citizen-Service Graph:} By loading an "Example Citizen" (a valid applicant instance), the system uses the Information Model to link the abstract Information Concepts from the Service Graph directly to the actual data nodes in the Citizen Graph via \texttt{ex:mapsTo} edges. This unified graph serves as a visual "audit trail," allowing human inspectors or automated agents to trace exactly which specific data points are being used to evaluate a specific constraint. 
\end{enumerate}
Both Graphs are serialized using Turtle syntax and saved to file as artifacts. Interactive visualizations of them are generated using the \textit{PyVis} library and also saved to file as \texttt{html} files. \par
Figure \ref{fig:citizen-service-example} shows a simplified view of a citizen-service graph as it was automatically produced by the pipeline. The yellow node is the \texttt{cpsv:PublicService} node, the red nodes are \texttt{cccev:Constraint} nodes, the blue nodes are \texttt{cccev:InformationConcept} nodes. These connect to the Citizen nodes (Green, the root, and grey, the properties) completing the Citizen-Service Graph.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/citizen-service-example.png}
    \caption{A citizen-service graph according to the Meta-Model}
    \label{fig:citizen-service-example}
\end{figure}

\subsubsection{Stage 4: SHACL Shapes Generation}
The final stage of the pipeline is responsible for synthesizing the executable validation logic. This stage transforms the abstract requirements from the Information Model into a strictly valid Shapes Constraint Language (SHACL) document. It should be noted that this is theoretically the most demanding task the LLM performs throughout this pipeline, from a semantic understanding perspective. The generation happens in two steps. \par
First, the system deterministically distills the rich Information Model into a simplified, noise-free JSON structure termed the "SHACL-Spec". This intermediate representation reorganizes the structure and retains only the logical primitives required for validation (e.g. rules, target paths and data types). This step acts as a "context cleaner", helping the LLM focus exclusively on code synthesis. \par
Second, the LLM is invoked to translate this specification into RDF triples (Turtle format). The prompt enforces a Dual-Strategy Protocol for logic synthesis. For atomic constraints involving single-hop properties and literal comparisons (e.g., \texttt{Citizenship = 'GR'}), the model generates standard \texttt{sh:property} shapes. SHACL-Core provides a rich set of structural and logical components (e.g., \texttt{sh:and}, \texttt{sh:or}, \texttt{sh:not}) and can encode fairly complex conditional logic and exception patterns \cite{Nuyts2024Comparative}. \par
For requirements involving arithmetic, aggregations, date comparisons or crossreferenced data (e.g., \texttt{now() $-$ birthDate $>$ 18}), the model encapsulates the logic within a  \texttt{SPARQL Constraint} node as per the standard and recommended approach in the literature \cite{Ferranti2024Formalizing}\cite{Pareti2021A}. This allows for the expression of complex conditional logic that exceeds the expressivity of the SHACL Core vocabulary \cite{Hagedorn2023Semantic}. The model is once again restricted to using the fixed Citizen Schema, which is once again given as context to act as a failsafe, in case earlier path generation failed to include crucial nodes. \par
As a last addition, the LLM generates an error message for every shape, which is intended to be displayed as part of the Validation Engine report in case of a violation (e.g., "Income exceeds threshold"). \par
The output is a fully serialized \texttt{ttl} file containing the \texttt{sh:NodeShape} definitions. This file serves as the executable input for the Validation Engine, the mechanics of which are detailed in the next section. \par
Listing \ref{lst:shacl-shape} shows a SHACL Shape and its embedded SPARQL query generated by this process. Specifically, it is the shape that corresponds to the constraint we showed in Listing \ref{lst:json-model} in its Information Model form.
\lstinputlisting[
    language=json, 
    caption={A SHACL Shape generated by the pipeline}, 
    label={lst:shacl-shape}
]{Snippets/shacl-shape.ttl}

\subsection{The Validation Engine} \label{subsec:validation_engine}
The final component of the architecture is the Validation Engine, which functions as the execution core of the system's symbolic layer. While previous stages focus on structuring and grounding the data, this engine is responsible for applying the generated constraints against specific citizen data to render a final, deterministic eligibility decision. \par
The engine operates on two distinct RDF graphs:
\begin{itemize} 
    \item \textbf{The Shapes Graph:} The \texttt{.ttl} file generated by Stage 4 of the pipeline, containing the \texttt{sh:NodeShape} definitions and SPARQL constraints within.
    \item \textbf{The Data Graph (Citizen Instance):} An RDF graph representing a specific applicant (a concrete instantiation of the Citizen Schema). It contains the factual assertions about an individual, structured strictly according to the domain ontology.
\end{itemize}
For the Execution and Reasoning step, the system utilizes PySHACL, a Python-based implementation of the W3C SHACL standard, to perform the validation. The execution follows a standard protocol: 
\begin{itemize} 
    \item \textbf{Targeting:} The engine identifies the "Focus Node" in the Data Graph (defined during Stage 4 as the sole instance of the class \texttt{:Applicant}). 
    \item \textbf{Constraint Evaluation:} For every Shape mapped to the Applicant, the engine evaluates the corresponding logic. Simple property shapes are validated via graph traversal, while complex conditions trigger the execution of the embedded SPARQL queries against the Data Graph. 
    \item \textbf{Entailment:} The engine operates under the RDFS entailment regime, allowing it to infer class hierarchies (e.g., understanding that a \texttt{:Child} is also a \texttt{:Person}) during validation.
\end{itemize}
The output of the engine is a formal \textit{Validation Report Graph} adhering to the SHACL standard. This report provides as output: 
\begin{enumerate} 
    \item \textbf{Boolean Conformance:} A global \texttt{sh:conforms} value (True/False), which serves as the system's final decision on eligibility. 
    \item \textbf{Violation Details:} A set of \texttt{sh:ValidationResult} nodes in cases of non conformance. Each one links to the specific Shape that failed and includes the generated error message, providing explanation for the rejection.
\end{enumerate}

\section{Experimental Design} \label{sec:experimental_design}
To evaluate the reliability, functional correctness and operational stability of the proposed architecture, an experimental framework was developed. The design moves beyond simple anecdotal testing, implementing a means to quantify the performance of the Neuro-Symbolic pipeline under varying conditions. \par
The core unit of the experiment is defined as a "run". A run represents a single end-to-end execution of the pipeline, characterized and configured by a specific combination of variables, dubbed the \textit{Configuration Tuple}:
\begin{quote}
    \texttt{(Document, Model, Prompting Strategy)}
\end{quote}
Even when generating executable code, Large Language Models are inherently non-deterministic when operating at non-zero temperature settings and can generate vastly different results given the exact same configuration (inputs, prompt, context) \cite{Ouyang2023An}. For some model architectures, even setting temperature to \textit{zero} reduces but \textit{does not eliminate} non-determinism \cite{Schäfer2023An}. Consequently, a single generation is insufficient to prove or disprove their reliability. \par
To address this, the framework executes a loop of multiple iterations for each unique configuration. In fact, empirical recommendations emphasize multiple generations to draw reliable conclusions \cite{Donato2025Studying}. This repetition allows for "drowning out" stochasticity and for the results metrics to converge to values that describe the actual stability of the pipeline with more fidelity. \par
The execution of these runs is done in The Orchestration Layer (see Section \ref{subsec:system_architecture}), which oversees the following lifecycle for every iteration:
\begin{enumerate} 
    \item \textbf{Context Initialization:} At the start of a run, a dictionary is initialized. This volatile data structure acts as a "flight recorder", accumulating outputs and metadata. 
    \item \textbf{Pipeline Execution:} The extraction and generation pipeline is triggered. If the pipeline encounters a critical failure, the failure mode is logged and the run is marked as incomplete before moving to the next.
    \item \textbf{Scenario Validation:} Upon successful generation of a valid SHACL graph, the system proceeds to the Mutation Testing phase (detailed in the following subsection), where the generated logic is stress-tested against a battery of specific, pre-made scenarios. 
    \item \textbf{Persistence:} Finally, the accumulated metrics are "flushed" to a CSV file. Results are persisted immediately to prevent data loss during long-running batch experiments.
\end{enumerate}

\subsection{The Mutation Testing Framework}
To evaluate the functional correctness of the generated SHACL shapes, the system implements a Mutation Testing Framework. Mutation testing has matured and gained popularity in evaluating test suites and supporting experimentation \cite{Papadakis2019Chapter}. Unlike traditional unit tests that might check for static string matches, this framework dynamically generates RDF graph instances to test whether the generated logic correctly distinguishes between eligible and ineligible applicants. The framework operates on a "Baseline and Perturbation" model, consisting of the components analyzed below.

\subsubsection{The "Golden Citizen" Baseline} 
For each public service document, a single, syntactically perfect RDF graph termed the \textit{Golden Citizen} is manually constructed. This data instance represents an applicant who satisfies \textit{all} eligibility preconditions. This baseline graph is constructed to adhere strictly to the Citizen Schema. The data values are calibrated to demonstrate marginal eligibility (e.g., if an income upper limit is €12,000, the Golden Citizen might have €11,999). This ensures that the testing framework evaluates the precision of the logic, not just its general functionality.

\subsubsection{Scenarios} 
The test cases (Scenarios) are defined in a declarative YAML configuration file. Each entry in this file represents a distinct Scenario, designed to isolate and test a specific logical constraint found in the document. A Scenario definition includes:
\begin{enumerate}
    \item \textbf{Expected Violation Count:} The ground truth for the test. A compliant scenario expects 0 violations, a failure scenario typically expects 1.
    \item \textbf{Mutation Actions:} A set of instructions to alter ("mutate") the Golden Citizen.
\end{enumerate}
Listing \ref{lst:scenario-yaml} provides an example of one mutation scenario.
\lstinputlisting[
    language=json, 
    caption={YAML description of a scenario}, 
    label={lst:scenario-yaml}
]{Snippets/scenario.yaml}
Crucially, mutations are designed to be atomic. Each scenario targets a single "fact" in the graph (e.g., changing a Literal value or a URI reference) to nudge the applicant from an "Eligible" state to a "Non-Eligible" state. This isolation allows the Validation Engine to pinpoint exactly which specific rule the LLM failed to generate correctly, if any. \par
For this work, all mutation scenarios were hand-crafted to reinforce confidence in the results. However, recent work has used LLMs to generate mutants, showing that mutation scores align with detection of real errors, extending classic mutation assumptions into LLM settings \cite{Tip2024LLMorpheus}.

\subsubsection{The Mutation Engine} 
For every iteration ("run"): 
\begin{enumerate} 
    \item The system loads the Golden Citizen graph into memory. 
    \item It creates a deep copy of the graph to ensure test isolation. 
    \item Once per scenario, it applies the Patch Logic. The engine parses the Turtle snippets defined in the YAML actions (e.g., \texttt{ex:Income :amount 12,000.1}) and updates the graph triples accordingly. This allows for complex graph transformations, such as replacing nodes or updating relationships, without manual RDF manipulation. 
\end{enumerate}
The resulting Mutated Citizen Graph and the Generated Shapes Graph (from Stage 4) are then passed to the afformentioned Validation Engine (section \ref{subsec:validation_engine}). The boolean outcome (\texttt{conforms}) and the number of violations with their associates messages are captured and logged to later be compared against the Expected Violation Count defined in the scenario.

\subsection{Experimental Configurations}
Recall the configuration tuple around which the experiment was designed:
\begin{quote}
    \texttt{(Document, Model, Prompting Strategy)}
\end{quote}
For this work we chose 2 documents, 2 models and 3 prompting strategies, for a total of 12 different experimental configurations. This combinatorial approach allows for the isolation of specific failure modes, distinguishing between errors caused by document complexity, model reasoning capacity, or prompting sufficiency. Below we analyze each component of the tuple and the configurations explored in the scope of this work.

\subsubsection{Document Corpora (Use Cases)} 
This selection tests the pipeline's ability to generalize across different domains and logical structures. This discrepancy was deemed necessary, as research indicates that LLMs often experience a non-linear degradation in performance, as reasoning depth and contextual density increase \cite{gupta2025novelhopqadiagnosingmultihopreasoning}. \par
Two public service documents were selected to represent two different levels of beurocratic complexity. 

\paragraph{Student Housing Allowance (High Complexity)} 
Selected as the "Stress Test" for the system. This document is characterized by: 
\begin{itemize} 
    \item \textbf{Deep Graph Traversal:} Verification requires traversing multiple hops (Applicant $\rightarrow$ Parents $\rightarrow$ Properties $\rightarrow$ Location). 
    \item \textbf{Recursive Arithmetic:} It involves dynamic income thresholds, calculated based on the count of dependent children (e.g., \textit{Limit = Base + (N $\times$ Bonus)}). 
    \item \textbf{Referential Integrity Constraints:} Verification requires comparing the identity of URI nodes rather than literal values (e.g., validating that the \texttt{:UniversityCity} node is distinct from the \texttt{:FamilyResidenceCity} node).
\end{itemize}

\paragraph{Special Parental Leave Allowance (Intermediate Complexity)}
Selected to evaluate standard administrative processing. This document focuses on:
\begin{itemize}
    \item \textbf{Categorical Classification:} Eligibility relies on specific enumerated values (e.g., Employment Sector must be "Private" or "Public").
    \item \textbf{Temporal Logic:} Involves duration calculations (e.g., "1 year of continuous employment") rather than complex arithmetic aggregations.
\end{itemize}

\subsubsection{Large Language Models} 
The experiment uses two of the models in the Google Gemini 2.5 family, to evaluate the trade-off between reasoning capability and computational efficiency. 
\begin{itemize} 
    \item \textbf{Gemini 2.5 Pro:} The high-parameter "reasoning" model. It is hypothesized to excel at complex SPARQL generation and abstracting vague requirements into formal logic, potentially at the cost of higher latency. 
    \item \textbf{Gemini 2.5 Flash:} The lightweight, low-latency model. It serves to test the feasibility of a "high-throughput" pipeline. A key research question is whether this smaller model can adhere to the strict SPARQL syntax requirements without the deep reasoning capabilities of the Pro variant.
\end{itemize}

\subsubsection{Prompting Strategies} 
Three distinct prompting strategies were implemented to evaluate the impact of "In-Context Learning" and "Self-Correction" on code quality.

\paragraph{Default Strategy (Few-Shot with Guardrails)} 
This strategy represents the baseline optimized approach. The system prompt instructs the model to act as an "Expert", a \textit{role-playing} strategy that can potentially improve answer quality for some models and tasks \cite{Chen2025Enhancing}. The prompt also provides: 
\begin{itemize} 
    \item \textbf{Proposed Strategy:} Explicit instructions to choose between, a simplification of the \textit{Plan-and-Solve} strategy \cite{Wang2023PlanandSolve}.
    \item \textbf{Syntactic Guardrails:} A set of negative constraints, derived from pilot testing frequent errors, as an attempt to improve alignment with syntactic expectations \cite{Chu2024A}.
    \item \textbf{Few-Shot Examples:} Concrete examples demonstrating correct and desired outputs, a method that has shown to consistently improve results with harder extraction tasks, especially when examples clarify desired structure \cite{Sivarajkumar2024An}.
\end{itemize}

\paragraph{Zero-Shot Strategy (Ablation Study)} 
To quantify the value of the engineering effort put into the Default prompt, the Zero-Shot strategy removes all Few-Shot Examples: the model is given the instructions but no reference implementations. This tests the model's innate reasoning prowess and knowledge of syntax versus its reliance on pattern matching from examples. This was also an attempt to see if diminishing returns in simple tasks and small context, observed in other studies \cite{Kresevic2024Optimization}, will affect our use cases.

\paragraph{Reflexion Strategy (Iterative Self-Correction)} 
This strategy implements a \textit{Prompt Chaining} loop that has the following steps: 
\begin{enumerate} 
    \item The model generates a draft response using the Default strategy. 
    \item The output is passed back to the model with a new "persona": \textit{"Senior Data Quality Assurance Auditor."} This agent is instructed to critique the quality of the draft with regards to criteria such as completeness, logical contradictions and syntactic validity. 
    \item If errors are found, the model rewrites the response based on its own critique. 
\end{enumerate} 
Listing \ref{lst:reflexion-prompt} shows an excerpt of the reflexion prompt.
\lstinputlisting[
    language=json, 
    caption={Excerpt of the reflexion prompt}, 
    label={lst:reflexion-prompt}
]{Snippets/reflexion.txt}
This configuration evaluates the efficacy of self-correction mechanisms in code generation, specifically testing whether the computational overhead of iterative refinement yields a statistically significant reduction in errors \cite{Liu2025InstructofReflection}.

\subsection{Evaluation Metrics}
To move beyond qualitative observation, the experimental framework was designed in such a way to capture a granular dataset for every execution cycle. This data collection strategy was designed to decouple structural failures (code that does not compile) from logical failures (code that compiles but yields incorrect decisions), enabling a multi-dimensional analysis of pipeline performance.

\subsubsection{Data Collection} 
For every experimental run, the system persists a dataset that captures the complete state of the pipeline at the moment of execution, categorized into five distinct dimensions:
\begin{itemize} 
    \item \textbf{Configuration Metadata:} Contextual fields regarding a unique Run ID, timestamp, the specific document input, the LLM employed and the active prompting strategy. 
    \item \textbf{Artifact Fingerprinting:} To track the stability and uniqueness of the LLM's output, the system computes and logs the cryptographic hashes (MD5) of the generated graphs. This allows for the detection of potentially identical artifacts generated across different runs.
    \item \textbf{Syntactic Integrity Verification:} Before execution, the system first verifies if the generated text is a valid RDF/Turtle graph (parsable by RDFLib), and secondly, it performs a "deep compile" check on every embedded SPARQL constraint to ensure the query syntax adheres to the SPARQL standard. Both errors, if they occur, are flagged differently to be distinguishable.
    \item \textbf{Validation Outcome Metrics:} The raw output of the validation engine is captured in detail. This includes the Actual Violation Count, the Expected Violation Count (derived from the scenario definition) and a serialized list of the specific Violated Shapes and their associated error messages. These fields enable the calculation of granular error metrics beyond simple binary accuracy. 
    \item \textbf{Operational Diagnostics:} To monitor system health, metrics such as end-to-end Execution Time (latency) and Runtime Error Messages (e.g., Python exceptions) are logged. These fields are critical for quantifying the operational stability of components such as the external API.
\end{itemize}

\subsubsection{Performance Indicators} 
The analysis of this dataset focuses on two primary dimensions of success.

\paragraph{Syntactic Validity} 
The first hurdle for any code-generating system is the production of executable syntax \cite{chen2021evaluatinglargelanguagemodels}. This metric quantifies the percentage of runs where the LLM produced a \texttt{.ttl} file that could be successfully parsed by the RDFLib graph library \textit{and} whose embedded SPARQL queries could be compiled without error. A run that fails this check is distinguished from runs that simply produce incorrect logic.

\paragraph{Functional Logic Accuracy} 
For runs that pass the syntax check, the focus shifts to logical fidelity. This is measured by comparing the system's eligibility decision against the known ground truth of the mutation scenarios. By treating the validation outcome as a binary classification task, where a "Conformance" is the Positive class and "Violation" is the Negative class, standard machine learning metrics can be calculated.

\section{Conclusion} 
This chapter has detailed the architectural and experimental foundations of the Neuro-Symbolic pipeline. By combining a schema-grounded generation process with a deterministic mutation testing framework, the system is designed to provide a quantifiable evaluation of LLM capabilities in the context of this task. The following chapter presents the results of these experiments, analyzing the pipeline's performance across the afformentioned dimensions.