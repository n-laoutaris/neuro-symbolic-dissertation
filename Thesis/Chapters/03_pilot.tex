\chapter{Pilot Study}

\section{Overview}
Present the end-to-end pipeline design in high-level, detailing the data flow from document ingestion to the final eligibility decision and the mutation framework. Flow chart here?
Unlike "end-to-end" approaches that attempt to generate code directly from text, this pipeline utilizes a chain of intermediate structured representations to stabilize the Large Language Model's reasoning process.

\section{Methodology and System Architecture}

\subsection{Setup Environment}
The pipeline was implemented using Python 3.12.9, utilizing a modular architecture to separate core processing logic from experimental orchestration. The system relies local processing for semantic graph operations and cloud-based APIs for Large Language Model inference.

\subsubsection{System Architecture} The codebase follows a functional separation of concerns, organized into three distinct layers: 
\begin{enumerate} 
    \item \textbf{The Core Module Library:} Contains the reusable logic, such as API communication, graph operations, parsing and testing utilities. It also contains the \textit{pipeline core}, which encapsulates the end-to-end extraction-generation workflow.
    \item \textbf{The Orchestration Layer (The "Cockpit"):} An interactive Jupyter Notebook serves as the control interface. This layer manages the experimental loop, injects configuration variables into the core modules and handles exceptions without interrupting batch processing.
    \item \textbf{The Persistence Layer:} To ensure auditability and reproducibility, the system employs a strict "Artifact Preservation" strategy. Every experimental run generates a dedicated directory locally, containing all intermediate outputs of the core pipeline. Testing metrics and metadata are saved in a Master CSV file for post-hoc analysis.
\end{enumerate}

\subsubsection{Technologies and Libraries} The system integrates standard Semantic Web technologies with modern Data Science tools: 
\begin{itemize} 
    \item \textbf{RDFLib:} Used for parsing, manipulating and serializing RDF graphs (Turtle format), as well as executing local SPARQL queries.
    \item \textbf{PySHACL:} The standard Python implementation of the SHACL validation engine, used to vaidate the LLM-generated shapes against the citizen data.
    \item \textbf{Pandas:} Used for the post-hoc aggregation and statistical analysis of the testing logs.
\end{itemize}

\subsection{Semantic Data Modelling} 
This pipeline was designed specifically with public service documents in mind. To bridge the gap between unstructured administrative text and deterministic validation logic, two distinct semantic layers were defined. These RDFS schemas serve as the symbolic "grounding" for the Large Language Model.

\subsubsection{The Public Service Meta-Model} \label{subsec:service_schema}
To ensure semantic interoperability and standardization, the modeling of the public service itself adheres to European formal vocabularies, specifically the Core Public Service Vocabulary Application Profile (CPSV-AP) and the Core Criterion and Evidence Vocabulary (CCCEV). The schema follows a hierarchical structure: 
\begin{itemize} 
    \item \textbf{cpsv:PublicService}: The root node representing the public service itself. 
    \item \textbf{cccev:Constraint}: Connected to the root node via \texttt{cpsv:holdsRequirement}, these nodes represent individual preconditions extracted from the text. 
    \item \textbf{cccev:InformationConcept}: Connected to Constraint nodes via \texttt{cccev:constrains}, these nodes represent the abstract information required to evaluate a constraint. 
\end{itemize} 
The adoption of established EU standards is a deliberate architectural choice, made to ensure cross-border interoperability and extensibility. By anchoring the pipeline's output in the CPSV-AP and CCCEV ecosystems, the generated graphs are natively compatible with the broader European e-Government infrastructure (such as the Single Digital Gateway). Furthermore, this modular design allows for future expansion where the pipeline could automatically ingest the full breadth of these ontologies (complex Evidence mappings, Agent definitions, Output representations), without requiring a fundamental restructuring of the core logic.

\subsubsection{Citizen Schema} \label{subsec:citizen_schema}
While the Public Service Meta-Model describes the \textit{rules}, the Citizen Schema describes the \textit{applicant}. This work utilizes a domain-specific RDFS schema tailored to the requirements of each document and generated in a separate workflow (not presented here) by the same LLM used in the implementation of the rest of the pipeline. The model is instructed to use granular instead of aggregate data as nodes (e.g. prefer "Date of Birth" rather than "Age") and is encourgaed to use abstract and reusable classes. \par
It has been demonstrated that the generation of such schemas can be automated as part of the pipeline (cite Konstantinidis). However, for the scope of this pilot study, the Citizen Schema is treated as fixed input context. This methodological choice serves two purposes: 
\begin{enumerate} 
    \item \textbf{Experimental Control:} By fixing the target schema, we isolate the performance of the LLM in \textit{logic generation} (SHACL/SPARQL) and \textit{extraction}, without the confounding variable of schema generation errors.
    \item \textbf{Prerequisite for Testing:} The automated testing framework relies on injecting specific faults into the citizen graph (e.g., modifying property values to trigger violations). This requires a deterministic, known-in-advance schema structure; had the schema been generated dynamically during each run, it would be impossible to define a static library of test scenarios targeting specific graph nodes. 
\end{enumerate}

\subsection{The Extraction and Generation Pipeline}
The core contribution of this work is the following multi-stage, neuro-symbolic pipeline. The process follows a sequential data flow, depicted in Figure \ref{fig:pipeline}, consisting of four primary  stages.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Pipeline.png}
    \caption{Flow Chart of the core pipeline.}
    \label{fig:pipeline}
\end{figure}

\subsubsection{Stage 1: Document Summarization and Precondition Extraction} 
The pipeline begins with the ingestion of the raw public service document (PDF). Using a Large Language Model (LLM), the unstructured text is processed to extract a summary of eligibility preconditions. The prompt is designed to filter out administrative noise and standardize the format of the rules. Summarization reduces the cognitive load required for the subsequent logic generation steps.

\subsubsection{Stage 2: Information Model Generation} 
In this critical neuro-symbolic step, the extracted preconditions are transformed into a structured JSON "Information Model". The Information Model organizes the unstructured rules into a strict hierarchy that mirrors the Meta-Model structure: 
\begin{itemize} 
    \item \textbf{Constraints:} Each eligibility rule is encapsulated as a Constraint object, containing the natural language description of the rule. 
    \item \textbf{Information Concepts:} Nested within each Constraint are the abstract Information Concepts, representing the specific pieces of evidence or data required to evaluate that rule. 
\end{itemize}
Inferring these concepts from the list of rules is the main reasoning task of the LLM at this stage. However, a second task it is prompted with is to act as a semantic mapper. The LLM is provided with the Citizen Schema (defined in Section \ref{subsec:citizen_schema}) as a strict vocabulary constraint to prevent the hallucination of non-existent properties. With it, it is instructed to connect each Information Concept with a number of Citizen nodes, by constructing specific traversal paths through the ontology (e.g., mapping the concept of "Applicant Age" to the path \texttt{:Applicant/:birthDate}). \par
The output is strictly enforced using a Pydantic schema definition, ensuring valid JSON structure. The resulting artifact effectively creates a "blueprint" for downstream tasks. It contains all the necessary semantic links to be deterministically serialized into valid CPSV/CCCEV triples in the subsequent stage, while ensuring that all data references are grounded in the controlled vocabulary of the Citizen Schema.

\subsubsection{Stage 3: Semantic Graph Construction}
Once the Information Model is established, the system deterministically (via Python code) constructs two RDF artifacts without further LLM inference:
\begin{enumerate} 
    \item \textbf{The Service Graph:} A formal representation of the public service using the CPSV-AP and CCCEV vocabularies and following the Meta-Model schema defined in Section \ref{subsec:service_schema}.
    \item \textbf{The Citizen-Service Graph (Explainability Layer):} By loading an "Example Citizen" (a valid applicant instance), the system uses the Information Model to link the abstract Information Concepts from the Service Graph directly to the actual data nodes in the Citizen Graph via \texttt{ex:mapsTo} edges. This unified graph serves as a visual "audit trail," allowing human inspectors or automated agents to trace exactly which specific data points are being used to evaluate a specific legal requirement.
\end{enumerate}
Both Graphs are serialized using \texttt{turtle} syntax and saved to file as artifacts. Interactive visualizations of them are generated using the \texttt{pyvis} library and also saved to file as \texttt{html} files.

\subsubsection{Stage 4: SHACL Shapes Generation}
The final stage of the pipeline is responsible for synthesizing the executable validation logic. This stage transforms the abstract requirements from the Information Model into a strictly valid Shapes Constraint Language (SHACL) document. \par
First, the system deterministically distills the rich Information Model into a simplified, noise-free JSON structure termed the "SHACL-Spec." This intermediate representation reorganizes the structure and retains only the logical primitives required for validation (e.g. rules, target paths and data types). This step acts as a "context cleaner", helping the LLM focus exclusively on code synthesis. \par
The LLM is then invoked to translate this specification into RDF triples (Turtle format). For atomic constraints involving single-hop properties and literal comparisons (e.g., \texttt{Citizenship = 'GR'}), the model generates standard \texttt{sh:property} shapes. For requirements involving arithmetic, aggregations, date comparisons, or cross-referenced data (e.g., \texttt{now() $-$ birthDate $>$ 18}), the model encapsulates the logic within \texttt{sh:sparql} constraints. This allows for the expression of complex conditional logic that exceeds the expressivity of the SHACL Core vocabulary. The model is once again restricted to using the fixed Citizen Schema, which is once again given as context to act as a failsafe, in case earlier path generation failed to include crucial nodes. \par
The output is a fully serialized \texttt{ttl} file containing the \texttt{sh:NodeShape} definitions. This file serves as the executable input for the Validation Engine, the mechanics of which are detailed in the following section.

\subsection{The Validation Engine}


\section{Experimental Design}

\subsection{The Mutation Testing Framework}
Describe the methodology of creating "Golden Citizens" and the YAML-based mutation engine used to systematically generate failing edge cases.

\subsection{Experimental Configurations}
Justify the selection of the specific documents, models, prompting strategies.

\subsection{Evaluation Metrics}
Describe what kind of data was colledted from the experiments. Define the specific quantitative metrics used, justify choosing them.

