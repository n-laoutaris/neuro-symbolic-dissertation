{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6206c7a1",
   "metadata": {},
   "source": [
    "# Experimental Results Analysis\n",
    "\n",
    "This notebook analyzes the outcomes from the neuro-symbolic experimentation framework, evaluating syntax reliability, logic accuracy and overall pipeline performance using data generated from automated SHACL synthesis and validation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b1fc1",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "This section imports necessary libraries for data analysis and visualization and sets up configuration parameters for plots and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777b28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for plot aesthetics and output directory\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "# Directory for autosaving images\n",
    "IMAGE_DIR = \"Thesis/Images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcbb85d",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Load the experimental results from the CSV file generated by the experimentation framework, and prepare the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba23a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the master results CSV generated from the experimentation runs\n",
    "df = pd.read_csv(\"Master_Results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400f297",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbb1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a runs dataframe by ignoring scenario-level details\n",
    "# Select only pipeline-related columns, excluding scenario specifics\n",
    "pipeline_columns = [\"Run ID\", \"Document Name\", \"Prompts\", \"Model Name\",\n",
    "    \"Service Graph Hash\", \"SHACL Graph Hash\", \"SHACL Valid Syntax\",\n",
    "    \"SHACL Error Type\", \"SHACL Error Message\", \"Successfully Executed\" ]\n",
    "\n",
    "# Remove duplicate rows based on Run ID to get one row per run (essentially dropping scenario-specifics)\n",
    "runs_df = df[pipeline_columns].drop_duplicates(subset=[\"Run ID\"])\n",
    "\n",
    "# Calculate total number of runs for context\n",
    "total_n_runs = len(runs_df)\n",
    "\n",
    "# Group runs by configuration (document, model, prompts)\n",
    "runs_grouped = runs_df.groupby([\"Document Name\", \"Model Name\", \"Prompts\"])\n",
    "\n",
    "# Display the number of experiments per configuration\n",
    "runs_grouped.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27089c87",
   "metadata": {},
   "source": [
    "## Syntax Validity\n",
    "\n",
    "Evaluate the reliability of SHACL code generation by measuring the percentage of runs that produce syntactically valid code and analyze common error types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80055d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of runs with syntactically valid SHACL\n",
    "valid_syntax_runs = runs_grouped[\"SHACL Valid Syntax\"].sum()    # Count valid runs per group\n",
    "total_runs = runs_grouped.size()    # Count total runs per group\n",
    "syntax_success_rate = (valid_syntax_runs / total_runs) * 100\n",
    "\n",
    "# Combine into a summary dataframe\n",
    "syntax_stats = pd.DataFrame({\n",
    "    \"Total Runs\": total_runs,\n",
    "    \"Valid Syntax Runs\": valid_syntax_runs,\n",
    "    \"Syntax Success Rate (%)\": syntax_success_rate\n",
    "}).reset_index()\n",
    "\n",
    "# Create a bar plot for syntax success rates\n",
    "g = sns.catplot(\n",
    "    data=syntax_stats,\n",
    "    kind=\"bar\",\n",
    "    x=\"Prompts\",\n",
    "    y=\"Syntax Success Rate (%)\",\n",
    "    hue=\"Model Name\",\n",
    "    col=\"Document Name\",\n",
    "    palette=\"viridis\",\n",
    "    height=5,\n",
    "    aspect=1.5,\n",
    "    order=[p for p in [\"Default\", \"ZeroShot\", \"Reflexion\"]] # Define order for x-axis\n",
    ")\n",
    "\n",
    "# Polish the plot\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Prompting Strategy\", \"Syntax Success Rate (%)\")\n",
    "g.set_titles(\"Document: {col_name}\")\n",
    "plt.ylim(0, 110)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for ax in g.axes.flat:\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.1f%%')\n",
    "\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.figure.suptitle(\"Syntax Reliability\")\n",
    "plt.savefig(f\"{IMAGE_DIR}/Syntax Reliability.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f379269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total runs per document and model\n",
    "runs_totals = runs_df.groupby([\"Document Name\", \"Model Name\"]).size().reset_index(name='Total')\n",
    "\n",
    "# Count occurrences of each error type\n",
    "error_counts = runs_df.groupby([\"Document Name\", \"Model Name\", \"SHACL Error Type\"]).size().reset_index(name='Count')\n",
    "\n",
    "# Merge and calculate error percentages\n",
    "error_stats = pd.merge(error_counts, runs_totals, on=[\"Document Name\", \"Model Name\"])\n",
    "error_stats['Percentage'] = (error_stats['Count'] / error_stats['Total']) * 100\n",
    "\n",
    "# Filter out valid runs to focus on errors\n",
    "error_stats = error_stats[error_stats['SHACL Error Type'] != 'VALID']\n",
    "\n",
    "# Plot error rates by type\n",
    "g = sns.catplot(\n",
    "    data=error_stats,\n",
    "    kind=\"bar\",\n",
    "    x=\"SHACL Error Type\",\n",
    "    y=\"Percentage\",\n",
    "    hue=\"Model Name\",\n",
    "    col=\"Document Name\",\n",
    "    palette=\"viridis\",\n",
    "    height=5,\n",
    "    aspect=1,\n",
    "    sharey=True # Keep y-axis consistent\n",
    ")\n",
    "\n",
    "# Polish the plot\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Error Category\", \"Error Rate (% of Total Runs)\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "\n",
    "# Add percentage labels\n",
    "for ax in g.axes.flat:\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.1f%%', padding=3)\n",
    "\n",
    "# Set title\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.figure.suptitle(\"Syntax Error Rates by Type\")\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"{IMAGE_DIR}/Syntax Error Analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0392c6a9",
   "metadata": {},
   "source": [
    "## Logic Validity\n",
    "\n",
    "Assess the accuracy of the generated SHACL constraints by checking if they correctly identify expected violations across all test scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only runs where the pipeline executed successfully\n",
    "valid_syntax_df = df[df[\"Successfully Executed\"] == 'True'].copy()\n",
    "\n",
    "# Check if each scenario matched expected violation count exactly\n",
    "valid_syntax_df['Strict_Scenario_Match'] = (\n",
    "    valid_syntax_df['Actual Violation Count'] == valid_syntax_df['Expected Violation Count']\n",
    ")\n",
    "\n",
    "# Aggregate by run: check if all scenarios in the run were perfect\n",
    "run_logic_df = valid_syntax_df.groupby(['Run ID', 'Document Name', 'Model Name', 'Prompts']).agg({\n",
    "    'Strict_Scenario_Match': 'all' # True only if all scenarios matched\n",
    "}).reset_index()\n",
    "\n",
    "# Rename column for clarity\n",
    "run_logic_df.rename(columns={'Strict_Scenario_Match': 'Perfect_Logic'}, inplace=True)\n",
    "\n",
    "# Calculate success rate per configuration\n",
    "logic_stats = run_logic_df.groupby(['Document Name', 'Model Name', 'Prompts'])['Perfect_Logic'].mean().reset_index()\n",
    "logic_stats['Logic Success Rate (%)'] = logic_stats['Perfect_Logic'] * 100\n",
    "\n",
    "# Create bar plot for logic success rates\n",
    "g = sns.catplot(\n",
    "    data=logic_stats,\n",
    "    kind=\"bar\",\n",
    "    x=\"Prompts\",\n",
    "    y='Logic Success Rate (%)',\n",
    "    hue=\"Model Name\",\n",
    "    col=\"Document Name\",\n",
    "    palette=\"magma\",\n",
    "    height=5,\n",
    "    aspect=1.5,\n",
    "    order=[p for p in [\"Default\", \"ZeroShot\", \"Reflexion\"]] # Define order for x-axis\n",
    ")\n",
    "\n",
    "# Polish the plot\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Prompting Strategy\", \"Logic Success Rate (%)\")\n",
    "g.set_titles(\"Document: {col_name}\")\n",
    "plt.ylim(0, 110)\n",
    "\n",
    "# Add percentage labels\n",
    "for ax in g.axes.flat:\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.1f%%')\n",
    "\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.figure.suptitle(\"Logic Reliability: % of Flawless Runs (All Scenarios Correct)\")\n",
    "plt.savefig(f\"{IMAGE_DIR}/Logic Reliability.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf2ee4",
   "metadata": {},
   "source": [
    "## Recommender System Performance Indicators\n",
    "\n",
    "Compute standard machine learning metrics like confusion matrix to evaluate the pipeline's feasibility as a recommender system for citizen eligibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ca46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only valid (successfully executed) runs\n",
    "valid_df = df[df[\"Successfully Executed\"] == 'True'].copy()\n",
    "\n",
    "# Define ground truth and predictions for recommender accuracy\n",
    "y_true = valid_df['Expected Violation Count'] == 0 # True if citizen is eligible (no expected violations)\n",
    "y_pred = valid_df['Actual Violation Count'] == 0 # True if system recommends eligibility (no actual violations)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[False, True]) # Labels: [Ineligible, Eligible]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Custom labels for clarity\n",
    "group_names = ['True Negative\\n(Correct Rejection)', 'False Positive\\n(Bad Recommendation)',\n",
    "               'False Negative\\n(Missed Opportunity)', 'True Positive\\n(Correct Recommendation)']\n",
    "\n",
    "# Flatten matrix to match names\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
    "\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=labels,\n",
    "    fmt='',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['Predicted Ineligible', 'Predicted Eligible'],\n",
    "    yticklabels=['Actually Ineligible', 'Actually Eligible']\n",
    ")\n",
    "\n",
    "plt.title(\"Overall Pipeline Feasibility: Recommender Accuracy\")\n",
    "plt.ylabel(\"Ground Truth (Citizen Status)\")\n",
    "plt.xlabel(\"System Output (Recommendation)\")\n",
    "plt.savefig(f\"{IMAGE_DIR}/Recommender Confusion Matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a9775",
   "metadata": {},
   "source": [
    "## Overall Reliability\n",
    "\n",
    "Categorize and visualize the distribution of run outcomes to understand the overall performance and failure modes of the experimentation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize the outcome of each run\n",
    "def categorize_run_outcome(group):\n",
    "    # 'group' is a DataFrame for one Run ID (all scenarios)\n",
    "\n",
    "    # Check if pipeline crashed\n",
    "    successfully_executed = str(group['Successfully Executed'].iloc[0])\n",
    "    if successfully_executed not in [\"True\", \"False\"]:\n",
    "        return \"Python Kernel Crash\"\n",
    "\n",
    "    # Check if SHACL compiled\n",
    "    syntax_error_type = group['SHACL Error Type'].iloc[0]\n",
    "    if syntax_error_type != \"VALID\":\n",
    "        return f\"{syntax_error_type.split('_')[0]} Syntax Error\" # Return error type\n",
    "\n",
    "    # Check if all logic tests passed\n",
    "    is_perfect = (group['Actual Violation Count'] == group['Expected Violation Count']).all()\n",
    "    if is_perfect:\n",
    "        return \"Perfect Run\"\n",
    "    else:\n",
    "        return \"Logic Failure\"\n",
    "\n",
    "# Apply categorization to each run\n",
    "run_outcomes = df.groupby(\"Run ID\").apply(categorize_run_outcome, include_groups=False) # Group by Run ID\n",
    "\n",
    "# Count occurrences of each outcome\n",
    "outcome_counts = run_outcomes.value_counts().reset_index()\n",
    "outcome_counts.columns = ['Outcome', 'Count']\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(\n",
    "    data=outcome_counts,\n",
    "    y=\"Outcome\",\n",
    "    x=\"Count\",\n",
    "    hue=\"Outcome\",\n",
    "    palette=\"coolwarm_r\" # Red for crashes, blue for perfect\n",
    ")\n",
    "\n",
    "# Polish the plot\n",
    "plt.title(f\"Pipeline Outcome Distribution (N={total_n_runs})\")\n",
    "plt.xlabel(f\"Number of Runs\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "# Add count labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, padding=3)\n",
    "\n",
    "plt.savefig(f\"{IMAGE_DIR}/Outcome Distribution.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
